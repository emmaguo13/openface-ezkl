{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import ezkl\n",
    "import os\n",
    "import json\n",
    "\n",
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "import torch\n",
    "from onnx2torch import convert\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from collections import OrderedDict\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "def ReadImage(image_path):\n",
    "    # Read the image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image from BGR to RGB\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert to PIL Image format\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "    # img_pil.show()\n",
    "    \n",
    "    # Detect and align the face\n",
    "    aligned_face, prob = mtcnn(img_pil, return_prob=True)\n",
    "\n",
    "    if aligned_face is not None:\n",
    "        # Convert the tensor to a numpy array and change from CHW to HWC format\n",
    "        print(aligned_face.shape)\n",
    "\n",
    "        aligned_face_np = aligned_face.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        # plt.imshow(aligned_face_np)\n",
    "\n",
    "        aligned_face_np = numpy.clip(aligned_face_np, 0, 1) * 255\n",
    "\n",
    "        aligned_face_np = aligned_face_np.astype(numpy.uint8)\n",
    "\n",
    "        # aligned_face_np = aligned_face.permute(1, 2, 0).mul(255).byte().numpy()\n",
    "\n",
    "        aligned_face_pil = Image.fromarray(aligned_face_np)\n",
    "        \n",
    "        # Display the PIL Image\n",
    "        # aligned_face_pil.show(title=f\"Aligned Face\")\n",
    "        \n",
    "        # Convert the numpy array to BGR format expected by OpenCV\n",
    "        aligned_face_bgr = cv2.cvtColor(aligned_face_np, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Resize the image to 96x96 pixels (OpenFace input size)\n",
    "        resized_face = cv2.resize(aligned_face_bgr, (96, 96))\n",
    "\n",
    "        # cv2.imshow(f'Aligned Face', resized_face)\n",
    "        \n",
    "        # Normalize the pixel values\n",
    "        resized_face = resized_face.astype(numpy.float32) / 255.0\n",
    "        \n",
    "        # Convert to PyTorch tensor and change from HWC to CHW format\n",
    "        aligned_face_tensor = torch.tensor(resized_face).permute(2, 0, 1).float()\n",
    "        \n",
    "        return aligned_face_tensor.unsqueeze(0)  # Add batch dimension\n",
    "    else:\n",
    "        print(f\"No face detected in {image_path}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "img_paths = ['emmapassport.png']\n",
    "imgs = []\n",
    "for img_path in img_paths:\n",
    "    imgs.append(ReadImage(img_path))\n",
    "\n",
    "I_ = torch.cat(imgs, 0)\n",
    "I_ = Variable(I_, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = (I_.detach().numpy()).reshape([-1]).tolist()\n",
    "data = dict(input_data = [data_array])\n",
    "data_path = os.path.join('input.json')\n",
    "settings_path = os.path.join('settings.json')\n",
    "model_path = os.path.join('openface_pt_native.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize data into file:\n",
    "json.dump( data, open(data_path, 'w' ))\n",
    "\n",
    "py_run_args = ezkl.PyRunArgs()\n",
    "py_run_args.input_visibility = \"private\"\n",
    "py_run_args.output_visibility = \"public\"\n",
    "py_run_args.param_visibility = \"fixed\" # private by default\n",
    "\n",
    "# What is the settings file?\n",
    "res = ezkl.gen_settings(model_path, settings_path, py_run_args=py_run_args)\n",
    "\n",
    "assert res == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_path = os.path.join(\"calibration.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 96, 96)\n",
      "(3, 96, 96)\n",
      "(3, 96, 96)\n",
      "(3, 96, 96)\n",
      "(3, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_paths = ['florentpassport.jpeg', \"kayleepassport.jpg\", \"emmapassport2.png\", \"florentselfie.jpg\", \"emmaselfie.png\"]\n",
    "imgs = []\n",
    "for img_path in img_paths:\n",
    "    imgs.append(ReadImage(img_path))\n",
    "\n",
    "I_ = torch.cat(imgs, 0)\n",
    "I_ = Variable(I_, requires_grad=False)\n",
    "\n",
    "data_array = (I_).reshape([-1]).tolist()\n",
    "data = dict(input_data = [data_array])\n",
    "\n",
    "json.dump(data, open(cal_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await ezkl.calibrate_settings(cal_path, model_path, settings_path, target=\"resources\", scales=[13], scale_rebase_multiplier=[1], only_range_check_rebase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model_path = os.path.join('network.compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "assert res == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await ezkl.get_srs( settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now generate the witness file \n",
    "witness_path = os.path.join('witness.json')\n",
    "pk_path = os.path.join('test.pk')\n",
    "vk_path = os.path.join('test.vk')\n",
    "\n",
    "res = await ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
    "assert os.path.isfile(witness_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PanicException",
     "evalue": "dynamic lookup or shuffle should only have one block",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPanicException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# HERE WE SETUP THE CIRCUIT PARAMS\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# WE GOT KEYS\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# WE GOT CIRCUIT PARAMETERS\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# EVERYTHING ANYONE HAS EVER NEEDED FOR ZK\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mezkl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiled_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvk_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpk_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m res \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(vk_path)\n",
      "\u001b[0;31mPanicException\u001b[0m: dynamic lookup or shuffle should only have one block"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calibration failed max lookup input (-4202994348144, 118871353118596) is too large\n",
      "Using 5 columns for non-linearity table.\n",
      "Using 5 columns for non-linearity table.\n",
      "Using 5 columns for non-linearity table.\n",
      "Using 5 columns for non-linearity table.\n",
      "Using 5 columns for non-linearity table.\n",
      "Using 5 columns for non-linearity table.\n",
      "Using 9 columns for non-linearity table.\n",
      "Using 9 columns for non-linearity table.\n",
      "Using 9 columns for non-linearity table.\n",
      "Using 9 columns for non-linearity table.\n",
      "Using 9 columns for non-linearity table.\n",
      "Using 9 columns for non-linearity table.\n",
      "calibration failed extended k is too large to accommodate the quotient polynomial with logrows 20\n",
      "circuit creation from run args failed: SigBitTruncationError\n",
      "circuit creation from run args failed: SigBitTruncationError\n",
      "calibration failed max lookup input (-78968271, 489000960) is too large\n",
      "calibration failed max lookup input (-67320924713427, 1897707960036100) is too large\n",
      "calibration failed max lookup input (-72087465, 488992968) is too large\n",
      "circuit creation from run args failed: SigBitTruncationError\n",
      "circuit creation from run args failed: SigBitTruncationError\n",
      "calibration failed max lookup input (-315677749, 1958141952) is too large\n"
     ]
    }
   ],
   "source": [
    "# HERE WE SETUP THE CIRCUIT PARAMS\n",
    "# WE GOT KEYS\n",
    "# WE GOT CIRCUIT PARAMETERS\n",
    "# EVERYTHING ANYONE HAS EVER NEEDED FOR ZK\n",
    "\n",
    "\n",
    "\n",
    "res = ezkl.setup(\n",
    "        compiled_model_path,\n",
    "        vk_path,\n",
    "        pk_path,\n",
    "        \n",
    "    )\n",
    "\n",
    "assert res == True\n",
    "assert os.path.isfile(vk_path)\n",
    "assert os.path.isfile(pk_path)\n",
    "assert os.path.isfile(settings_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ezkl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
